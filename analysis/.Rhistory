geom_point(color = "yellow1", alpha = 0.5) +
geom_abline(linetype = 2, colour = "cyan") +
geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")),
parse = TRUE, size = 2.8, label.y = 0.9, coef.digits = 3, rr.digits = 4) +
coord_cartesian(ylim=c(400, 800), xlim=c(400,800)) +
scale_x_continuous("predicted reaction time (ms)") +
scale_y_continuous("empirical mean reaction time (ms)") +
scale_colour_manual(values = "yellow1") -> orthog_plot
rt_pred_best <- map_dfr(exps_to_predict, predict_rt, 'best feature')
d %>% filter(exp_id %in% exps_to_predict, N_T > 0) %>%
group_by(exp_id, p_id, d_feature, N_T) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id,  d_feature, N_T) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") %>%
left_join(rt_pred_best, by = c("exp_id", "d_feature", "N_T")) %>%
ggplot(aes(x = p_rt, y = mean_rt, colour = "yellow1")) +
geom_point(color = "yellow1", alpha = 0.5) +
geom_abline(linetype = 2, colour = "cyan") +
geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")),
parse = TRUE, size = 2.8, label.y = 0.9, coef.digits = 3, rr.digits = 4) +
coord_cartesian(ylim=c(400, 800), xlim=c(400,800)) +
scale_x_continuous("predicted reaction time (ms)") +
scale_y_continuous("empirical mean reaction time (ms)") +
scale_colour_manual(values = "yellow1") -> best_plot
best_plot + orthog_plot + collinear_plot
# remove variables we no longer need
rm(De, Dp, rt_pred_collinear, rt_pred_orthog, rt_pred_best, exps_to_predict,
extract_a_value, extract_D, predict_rt)
De <- map_dfr(c("1a", "1b"), calc_D_per_feature, "Bayes")
De
De <- map_dfr(c("1a", "1b", "2a", "2b", "2c"), calc_D_per_feature, "Bayes")
saveRDS(De, "tmp.model")
De %>%
mutate(
d_feature = as_factor(d_feature),
d_feature = fct_relevel(d_feature, "yellow", "orange", "blue", "triangle", "semicircle", "diamond", "circle")) %>%
arrange(exp_id, d_feature) %>%
group_by(exp_id, d_feature) %>%
mean_hdci(D, .width = 0.53) %>%
select(exp_id, d_feature, `53% lower` = ".lower", D, `53% upper` = ".upper") %>%
knitr::kable() %>% kableExtra::kable_styling()
exps_to_predict <- c("2a")
Dp <- map_df(exps_to_predict, gen_exp_predictions, De, "Bayes")
Dp
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2)
Dp
filter(De, exp_id %in% exps_to_predict)
experiment = "2a"
df <- filter(d, exp_id == experiment) %>%
mutate(d_feature = fct_drop(d_feature))
prev_exp_number <- parse_number(experiment) - 1
prev_exps <- c(
paste(prev_exp_number, "a", sep = ""),
paste(prev_exp_number, "b", sep = ""))
D <- filter(De, exp_id %in% prev_exps)
f = levels(df$d_feature)[1]
# f is a feature condition, such as "blue circle"
# D is the dataframe that is output by calc_D_per_feature
f1 <- word(f, 1)
f2 <- word(f, 2)
D1 = as.numeric(filter(D, d_feature == f1)$D)
D1
D2 = as.numeric(filter(D, d_feature == f2)$D)
D_collinear = 1/((1/D1) + (1/D2))
D_best_feature = min(D1, D2)
D_orth_contrast =  sqrt(1/((1/D1^2) + (1/D2^2)))
df
d
De
d_out <- tibble(
exp_id = experiment,
iter = De$iter,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
d_out <- tibble(
exp_id = experiment,
iter = D$iter,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
d_out <- tibble(
exp_id = experiment,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
d_out
D
predict_D_overall <- function(f, D, approach = "freq")
{
# f is a feature condition, such as "blue circle"
# D is the dataframe that is output by calc_D_per_feature
f1 <- word(f, 1)
f2 <- word(f, 2)
D1 = as.numeric(filter(D, d_feature == f1)$D)
D2 = as.numeric(filter(D, d_feature == f2)$D)
D_collinear = 1/((1/D1) + (1/D2))
D_best_feature = min(D1, D2)
D_orth_contrast =  sqrt(1/((1/D1^2) + (1/D2^2)))
return(tibble(
d_feature = f,
iter = D$iter,
"best feature" = D_best_feature,
"orthog. contrast" = D_orth_contrast,
"collinear" = D_collinear))
}
d_out <- tibble(
exp_id = experiment,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
return(tibble(
d_feature = f,
iter = n(),
"best feature" = D_best_feature,
"orthog. contrast" = D_orth_contrast,
"collinear" = D_collinear))
predict_D_overall <- function(f, D, approach = "freq")
{
# f is a feature condition, such as "blue circle"
# D is the dataframe that is output by calc_D_per_feature
f1 <- word(f, 1)
f2 <- word(f, 2)
D1 = as.numeric(filter(D, d_feature == f1)$D)
D2 = as.numeric(filter(D, d_feature == f2)$D)
D_collinear = 1/((1/D1) + (1/D2))
D_best_feature = min(D1, D2)
D_orth_contrast =  sqrt(1/((1/D1^2) + (1/D2^2)))
return(tibble(
d_feature = f,
iter = n(),
"best feature" = D_best_feature,
"orthog. contrast" = D_orth_contrast,
"collinear" = D_collinear))
}
d_out <- tibble(
exp_id = experiment,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
predict_D_overall <- function(f, D, approach = "freq")
{
# f is a feature condition, such as "blue circle"
# D is the dataframe that is output by calc_D_per_feature
f1 <- word(f, 1)
f2 <- word(f, 2)
D1 = as.numeric(filter(D, d_feature == f1)$D)
D2 = as.numeric(filter(D, d_feature == f2)$D)
D_collinear = 1/((1/D1) + (1/D2))
D_best_feature = min(D1, D2)
D_orth_contrast =  sqrt(1/((1/D1^2) + (1/D2^2)))
return(tibble(
d_feature = f,
iter = 1:length(D1),
"best feature" = D_best_feature,
"orthog. contrast" = D_orth_contrast,
"collinear" = D_collinear))
}
d_out <- tibble(
exp_id = experiment,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
gen_exp_predictions <- function(experiment, De, approach = "freq")
{
# Predict values of D for composite features
df <- filter(d, exp_id == experiment) %>%
mutate(d_feature = fct_drop(d_feature))
prev_exp_number <- parse_number(experiment) - 1
prev_exps <- c(
paste(prev_exp_number, "a", sep = ""),
paste(prev_exp_number, "b", sep = ""))
D <- filter(De, exp_id %in% prev_exps)
d_out <- tibble(
exp_id = experiment,
map_dfr(levels(df$d_feature), predict_D_overall, D, approach))
return(d_out)
}
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2)
Dp <- map_df(exps_to_predict, gen_exp_predictions, De, "Bayes")
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2)
Dp
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2)
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear),
names_to = "method", values_to = "Dp")
group_by(d_feature, method) %>%
mean_hdci(.width = 0.53)
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear),
names_to = "method", values_to = "Dp") %>%
group_by(d_feature, method) %>%
mean_hdci(.width = 0.53)
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear),
names_to = "method", values_to = "Dp")
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(d_feature, method) %>%
mean_hdci(.width = 0.53)
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(d_feature, method) %>%
mean_hdci()
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(d_feature, method)
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(exp_id, d_feature, method) %>%
mean_hdci()
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(exp_id, d_feature, method) %>%
mean_hdci() %>%
pivot_wider(names_from = "method", values_from = "Dp")
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(exp_id, d_feature, method) %>%
mean_hdci() %>%
ungroup() %>%
pivot_wider(names_from = "method", values_from = "Dp")
Dp %>% mutate(
d_feature2 = d_feature,
d_feature = str_replace(d_feature, " ", "")) %>%
full_join(filter(De, exp_id %in% exps_to_predict), by = c("exp_id", "d_feature", "iter")) %>%
mutate(d_feature = d_feature2) %>%
select(-d_feature2, -iter) %>%
pivot_longer(
c(`best feature`, `orthog. contrast`, collinear, D),
names_to = "method", values_to = "Dp") %>%
group_by(exp_id, d_feature, method) %>%
mean_hdci()
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
#plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
#plt_log <- plot_model_fits_rt(1, prior_model_log, y_limits = c(0, 10), feature2plot = "blue")
#plt_sft <- plot_model_fits_rt(1, prior_model_sft, y_limits = c(0, 10), feature2plot = "blue")
plt_nrl + plt_log + plt_sft
training_models = c("1a", "1b")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
m_exp1_nrl <- readRDS("models/exp_1_nrl.models")
m_exp1_log <- readRDS("models/exp_1_log.models")
m_exp1_sft <- readRDS("models/exp_1_sft.models")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1:10, N_T = full_seq(N_T,2))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id, N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
exp(-1.5)
exp(1.5)
exp(1)
exp(-10)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggthemes)
library(brms)
library(tidybayes)
dat <- read_delim("../hori split/anna_data/Rtacc.txt", delim = "\t")
?summarise
setwd("~/GitHub/simpleConjunction/analysis")
library(tidyverse)
library(brms)
library(tidybayes)
d <- read_csv("../data_prolific/accuracy_rt_data.txt") %>%
mutate(
targ_present = as_factor(targPresent),
targ_present = fct_recode(targ_present, present = "1", absent = "0"),
trial_type = as_factor(targetType),
difficulty = as.numeric(as_factor(difficulty))) %>%
select(observer, trial="trial_type", targ="targ_present", difficulty, n, rt, accuracy)
d$response <- NA
d$response[which(d$targ=="present" & d$accuracy==1)] = 1
d$response[which(d$targ=="present" & d$accuracy==0)] = 0
d$response[which(d$targ=="absent" & d$accuracy==1)] = 0
d$response[which(d$targ=="absent" & d$accuracy==0)] = 1
summary(d)
# remove some outliers
d <- filter(d, rt > 0.2, rt < 20)
d <- filter(d, difficulty == 1)
formula <- bf(rt | dec(response) ~
targ:trial  + targ:trial:difficulty  +  (0 + targ:trial|p|observer),
bs ~ 0 + trial + (0 + trial|p|observer),
ndt ~ 0 + trial,
bias ~ 0 + trial + (0 + trial|p|observer))
get_prior(formula,
data = d,
family = wiener(link_bs = "identity",
link_ndt = "identity",
link_bias = "identity"))
prior <- c(
prior("cauchy(0, 5)", class = "b"),
set_prior("normal(1.5, 1)", class = "b", dpar = "bs"),
set_prior("normal(0.2, 0.1)", class = "b", dpar = "ndt"),
set_prior("normal(0.5, 0.2)", class = "b", dpar = "bias")
)
make_stancode(formula,
family = wiener(link_bs = "identity",
link_ndt = "identity",
link_bias = "identity"),
data = d,
prior = prior)
tmp_dat <- make_standata(formula,
family = wiener(link_bs = "identity",
link_ndt = "identity",
link_bias = "identity"),
data = d, prior = prior)
str(tmp_dat, 1, give.attr = FALSE)
initfun <- function() {
list(
b = rnorm(tmp_dat$K-1),
b_bs = runif(tmp_dat$K_bs, 1, 2),
b_ndt = runif(tmp_dat$K_ndt, 0.01, 0.1),
b_bias = rnorm(tmp_dat$K_bias, 0.5, 0.1),
sd_1 = runif(tmp_dat$M_1, 0.5, 1),
z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
tmp_dat$M_1, tmp_dat$N_1),
L_1 = diag(tmp_dat$M_1)
)
}
fit_wiener <- brm(formula,
data = d,
family = wiener(link_bs = "identity",
link_ndt = "identity",
link_bias = "identity"),
prior = prior,
inits = initfun,
iter = 1000,
chains = 4, cores = 4,
control = list(max_treedepth = 15))
saveRDS(fit_wiener, "tmp.model")
cor_idx <- get_variables(fit_wiener) %>% str_starts("cor")
cor_name <- get_variables(fit_wiener) [cor_idx]
fit_wiener %>% gather_draws(`cor.*`, regex = TRUE) %>%
rename(var = ".variable") %>%
mutate(var = str_remove(var, "cor_observer__")) -> cor_post
paradigms <- "(tex|line|greenVertical)"
vars <- c("targpresent:trial", "targabsent:trial", "bias_trial", "bs_trial" )
comparisons <-  paste(vars, paradigms, "__", vars, paradigms, sep = "")
cor_post %>% group_by(var) %>%
summarise(p_not_zero = mean(abs(.value) > 0.1)) %>%
mutate(p_not_zero = round(p_not_zero, 1),
p_not_zero = as.factor(p_not_zero)) %>%
full_join(cor_post) %>%
filter(str_detect(var, comparisons))-> cor_post
cor_post %>% separate(var, into = c("var", "paradigm1", "var2", "paradigm2")) %>%
mutate(comparison = paste(paradigm1, paradigm2)) %>%
select(var, comparison, .value, p_not_zero) -> cor_post
ggplot(cor_post, aes(x = .value, y= var, fill = p_not_zero)) +
geom_vline(xintercept = 0, linetype = 2) +
ggridges::geom_density_ridges(alpha = 0.5) +
facet_wrap(~comparison) +
scale_fill_viridis_d() +
theme_bw()
NPRED <- 1
pred_wiener <- predict(fit_wiener,
summary = FALSE,
negative_rt = TRUE,
nsamples = NPRED,
re_formula = NULL)
d$p <- as.numeric(pred_wiener)
d %>% mutate(rt = if_else(response == 0, -rt, rt)) %>%
pivot_longer(c(rt, p), names_to = "measure", values_to = "rt") %>%
ggplot(aes(x = rt, fill = measure)) +
geom_vline(xintercept = 0) +
geom_density(alpha = 0.3) +
facet_wrap(targ ~ trial, scales="free")
fit_wiener
