---
title: "Pilot Data Summary"
author: "A Clarke"
date: "20/05/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggthemes)
library(brms)
#library(tidybayes)
library(corrr)
library(patchwork)

options(digits = 2, mc.cores = 10)

knitr::opts_chunk$set(echo = TRUE)
```

# Data check and merge

## Import Data

Data froms from proficio (data Anna?) and Naa Cato (Alasdair's MSc student) 

```{r echo=FALSE}
my_cols <- cols(
  observer = col_character(),
  data_from = col_character(),
  trial = col_character(),
  difficulty = col_character(),
  targ = col_character(),
  rt = col_double(),
  accuracy = col_double()
)
```

```{r}
d <- read_csv("../pilot_data/pilot_data.csv", col_types = my_cols) %>%
  glimpse() %>%
  mutate(difficulty = as_factor(difficulty),
         difficulty = fct_relevel(difficulty, "easy", "mid", "hard"))
```

## Check Summary Stats Between Sites

### Accuracy

From looking at the accuracy data, Naa's data looks similar to our other pilot data so no reason not to merge.

```{r}
d %>% group_by(observer, data_from) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop") %>%
  ggplot(aes(x = observer, y = accuracy, fill = data_from)) + 
  geom_col() + 
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  ggthemes::scale_fill_pander() 
```

Observer 3's overall accuracy is at chance level, so I suggest we remove

```{r}
d <- filter(d, observer != "obs3")
```

Now check how people look in the easy condition across tasks. 

```{r}
d %>% filter(targ == "absent", difficulty == "easy") %>%
  group_by(observer, difficulty, trial) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop") %>%
  ggplot(aes(x = observer, y = accuracy)) + 
  geom_col(position = position_dodge()) + 
  facet_wrap(~trial, ncol = 1) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggthemes::scale_fill_pander() + 
  ggtitle("Easy Target Absent Trials")
```
Suggested exclusion rule: anybody who scores less than 75% in any one of the three target absent (easy difficulty) conditions. 

```{r}
d %>% filter(targ == "absent", difficulty == "easy") %>%
  group_by(observer, difficulty, trial) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop") %>%
  filter(accuracy < 0.75) %>%
  unite("key", observer, trial) -> to_remove

d %>% unite("key", observer, trial, remove = FALSE) %>%
  filter(!(key %in% to_remove$key)) %>%
  select(-key) -> d

rm(to_remove)
```

### Median Reaction Time

Now we will check that RTs are broadly similar across sites. Remember to only take correct trials!

```{r}
d %>% filter(accuracy == 1) %>%
  group_by(observer, data_from, trial, targ, difficulty) %>%
  summarise(med_rt = median(rt), .groups = "drop") %>%
  mutate(difficulty = as.numeric(difficulty)) %>%
  ggplot(aes(x = difficulty, y = med_rt, colour = data_from)) + 
  geom_jitter(height = 0) + 
  facet_grid(targ~trial) + 
  ggthemes::scale_color_pander()
```

Conclude that it is fine to merge pilot datasets?


## Checking for Outliers

Fancy models can have issues when there are outliers (unless you make an even fancier mixture model!)

Let's look at the min RT per person

```{r}
d %>% group_by(observer) %>%
  summarise(min_rt = min(rt), .groups = "drop") %>% 
  ggplot(aes(x = min_rt)) + geom_histogram(bins = 20, colour = "black") +
  geom_vline(xintercept = 0.250, linetype = 2) + 
  ggtitle("min RT per observer")
```

Removing all RTs < 0.250 seconds seems like a reasonable option

Now let's look at the max rt...

```{r}
d %>% group_by(observer) %>%
  summarise(max_rt = max(rt), .groups = "drop") %>% 
  ggplot(aes(x = max_rt)) + geom_histogram(bins = 20, colour = "black") +
  geom_vline(xintercept = 250, linetype = 2) +
  ggtitle("max RT per observer") + 
  scale_x_log10()
```
So perhaps take 250 seconds as our max RT cut off?

How doe these numbers look when compared to the histogram of all RTs.

```{r}
ggplot(d, aes(x = rt)) + geom_histogram(bins = 50) +
  scale_x_log10() + 
  geom_vline(xintercept = c(0.250, 250), linetype = 2) +
  ggtitle("all RTs")
```

The above seems reasonable to me.

```{r}
d %>% 
  summarise(percent_below = 100*mean(rt<0.250),
            percent_above = 100*mean(rt>250),
            percent_removed = percent_below + percent_above) %>%
  knitr::kable()

d <- filter(d, rt>0.250, rt<250)
```


## Output for Modelling

We have already removed 
- the participant who was very inaccurate
- RTs < 0.250 seconds (0.2% of data)
- RTs > 250.0 seconds (0.06% of data)

Before we run our model, we have to recover the present/absent responses made for each trial (rather than accuracy)

```{r}
d$response <- NA
d$response[which(d$targ=="present" & d$accuracy==1)] = 1
d$response[which(d$targ=="present" & d$accuracy==0)] = 0
d$response[which(d$targ=="absent" & d$accuracy==1)] = 0
d$response[which(d$targ=="absent" & d$accuracy==0)] = 1
```

Finally, save the data before modelling.

```{r}
write_csv(d, "../cluster/data_for_model.csv")
```


# Summary Statistics 

Let's see if there is a correlation between accuracy or rt!

## Accuracy

To start with, we'll ignore *difficulty*


```{r}
d %>% group_by(trial, targ, observer) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop_last") %>%
  unite("condition", c(trial, targ)) %>%
  pivot_wider(names_from = "condition", values_from = "accuracy") %>%
  select(-observer) %>%
  correlate(method = "pearson", use = "pairwise.complete.obs") %>%
  knitr::kable()

d %>% group_by(trial, targ, observer) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop") %>%
  pivot_wider(names_from = "trial", values_from = "accuracy") -> dplt


plt1 <- ggplot(dplt, aes(x = conj, y = ori)) + geom_point(alpha = 0.5) +
  facet_wrap(~targ, ncol = 1) + ggtitle("conjunction - orientation")

plt2 <- ggplot(dplt, aes(x = conj, y = tex)) + geom_point(alpha = 0.5) +
  facet_wrap(~targ, ncol = 1) + ggtitle("conjunction - texture")

plt3 <- ggplot(dplt, aes(x = ori, y = tex)) + geom_point(alpha = 0.5) +
  facet_wrap(~targ, ncol = 1) + ggtitle("orientation - texture")

plt1 + plt2 + plt3

```

# Modelling

## Forumla

```{r}
formula <- bf(rt | dec(response) ~
              # the first part is the linear predictor for drift rate
              0 + targ:trial  + targ:trial:difficulty  +
                (0 + targ:trial + targ:trial:difficulty|p|observer),
              # bs: boundary separation
              bs ~ 0 + trial + (0 + trial|p|observer),
              # ndt: non-decision time
              ndt ~ 0 + trial + (1|p|observer),
              # bias: starting bias for TA or TP
              bias ~ 0 + trial + (0 + trial|p|observer))
```

## Prior

Print out list of all priors required for our formula, and any defaults that are already in place:
\tiny
```{r, eval=FALSE}
get_prior(formula,
          data = d,
          family = wiener(link_bs = "log",
                          link_bias = "logit",
                          link_ndt = "log"))
```

Specify some of our own priors for the fixed effects:

\normalsize
```{r}
prior <- c(
  set_prior("normal(0, 3)", class = "b"),
  set_prior("normal(1.5, 1)", class = "b", dpar = "bs"),
  set_prior("normal(0, 0.01)", class = "sd", dpar = "ndt"),
  set_prior("normal(0.0, 0.5)", class = "b", dpar = "bias"),
  set_prior("normal(-5, 0.5)", class = "Intercept", dpar = "ndt"))
```

## Posterior

The model was fit on the cluster using:

```{r, eval = FALSE}
fit_wiener <- brm(formula,
                  data = d,
                  family = wiener(link_bs = "log",
                                  link_bias = "logit",
                                  link_ndt = "log"),
                  prior = prior,
                  inits = initfun,
                  iter = 2000,
                  chains = 1,
                  control = list(max_treedepth = 15, adapt_delta = 0.9))
```


Let's read in the fitted model

```{r}
fit_wiener <- readRDS("models/wiener_fit.model")
```

Check rhat statistics are close to 1

```{r}
hist(rhat(fit_wiener))
```

Max rhat is < 1.01, so no reason to be worried. 

### Posterior Predictions

For now, only take 10 samples from the posterior, but we can increase this later if we wish

```{r, eval = FALSE}
NPRED <- 5
pred_wiener <- predict(fit_wiener, 
                       summary = FALSE, 
                       negative_rt = TRUE, 
                       nsamples = NPRED,
                       re_formula = NULL)
d$p <- as.numeric(apply(pred_wiener, 2, median)) # this is dumb. Fix!
rm(pred_wiener)

## plot accuracy
d %>% mutate(
  p_rt = if_else(targ == "absent", -p, p),
  p_re = if_else(p < 0, 0, 1),
  p_ac = if_else((targ=="present")==p_re, 1, 0)) -> d

d %>%
  group_by(observer, trial, targ, difficulty) %>%
  summarise(
    accuracy = mean(accuracy),
    pred_acc = mean(p_ac)) %>%
  ggplot(aes(x = pred_acc, y = accuracy, colour = difficulty)) +
  geom_abline(linetype = 2) + 
  geom_jitter(alpha = 0.5, position = "jitter") +
  geom_smooth(method = "lm", formula = y~x, aes(group = 1), colour = "grey") + 
  facet_grid(targ~trial, scales = "free") +
  ggthemes::scale_colour_ptol() 
ggsave("model_pred_acc.png", height = 4, width = 6)


## plot rt
d %>% filter(accuracy == 1) %>%
  group_by(observer, trial, targ, difficulty) %>%
  summarise(empirical = median(rt)) -> ed

d %>% filter(p_ac == 1)  %>%
  group_by(observer, trial, targ, difficulty) %>%
  summarise(model = median(rt)) -> pd

full_join(ed, pd) %>%
  ggplot(aes(x = model, y = empirical, colour = difficulty)) +
  geom_abline(linetype = 2) + 
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y~x, aes(group = 1), colour = "grey") + 
  facet_grid(targ~trial) +
  coord_fixed() + 
  ggthemes::scale_colour_ptol() 
ggsave("model_pred_rt.png", height = 4, width = 6)
```
