---
title: "Pilot Data Summary"
author: "A Clarke"
date: "20/05/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggthemes)
library(brms)
library(tidybayes)

options(digits = 2, mc.cores = 10)

knitr::opts_chunk$set(echo = TRUE)
```

# Summary Data

## Import Data

Data froms from proficio (data Anna?) and Naa Cato (Alasdair's MSc student) 

```{r echo=FALSE}
my_cols <- cols(
  observer = col_character(),
  data_from = col_character(),
  trial = col_character(),
  difficulty = col_character(),
  targ = col_character(),
  rt = col_double(),
  accuracy = col_double()
)
```

```{r}
d <- read_csv("../pilot_data/pilot_data.csv", col_types = my_cols) %>%
  glimpse() %>%
  mutate(difficulty = as_factor(difficulty),
         difficulty = fct_relevel(difficulty, "easy", "mid", "hard"))
```

## Check Summary Stats Between Sites

### Accuracy

From looking at the accuracy data, Naa's data looks similar to our other pilot data so no reason not to merge.

```{r}
d %>% group_by(observer, data_from) %>%
  summarise(accuracy = mean(accuracy), .groups = "drop") %>%
  ggplot(aes(x = observer, y = accuracy, fill = data_from)) + 
  geom_col() + 
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  ggthemes::scale_fill_pander() 
```

Observer 3's overall accuracy is at chance level, so I suggest we remove

```{r}
d <- filter(d, observer != "obs3")
```

### Median Reaction Time

Now we will check that RTs are broadly similar across sites. Remember to only take correct trials!

```{r}
d %>% filter(accuracy == 1) %>%
  group_by(observer, data_from, trial, targ, difficulty) %>%
  summarise(med_rt = median(rt), .groups = "drop") %>%
  mutate(difficulty = as.numeric(difficulty)) %>%
  ggplot(aes(x = difficulty, y = med_rt, colour = data_from)) + 
  geom_jitter(height = 0) + 
  facet_grid(targ~trial) + 
  ggthemes::scale_color_pander()
```
Conclude that it is fine to merge pilot datasets?


## Checking for Outliers

Fancy models can have issues when there are outliers (unless you make an even fancier mixture model!)

Let's look at the min RT per person

```{r}
d %>% group_by(observer) %>%
  summarise(min_rt = min(rt), .groups = "drop") %>% 
  ggplot(aes(x = min_rt)) + geom_histogram(bins = 20, colour = "black") +
  geom_vline(xintercept = 0.250, linetype = 2) + 
  ggtitle("min RT per observer")
```

Removing all RTs < 0.250 seconds seems like a reasonable option

Now let's look at the max rt...

```{r}
d %>% group_by(observer) %>%
  summarise(max_rt = max(rt), .groups = "drop") %>% 
  ggplot(aes(x = max_rt)) + geom_histogram(bins = 20, colour = "black") +
  geom_vline(xintercept = 250, linetype = 2) +
  ggtitle("max RT per observer") + 
  scale_x_log10()
```
So perhaps take 250 seconds as our max RT cut off?

How doe these numbers look when compared to the histogram of all RTs.

```{r}
ggplot(d, aes(x = rt)) + geom_histogram(bins = 50) +
  scale_x_log10() + 
  geom_vline(xintercept = c(0.250, 250), linetype = 2) +
  ggtitle("all RTs")
```

The above seems reasonable to me.

```{r}
d %>% 
  summarise(percent_below = 100*mean(rt<0.250),
            percent_above = 100*mean(rt>250),
            percent_removed = percent_below + percent_above) %>%
  knitr::kable()

d <- filter(d, rt>0.250, rt<250)
```


## Output for Modelling

We have already removed 
- the participant who was very inaccurate
- RTs < 0.250 seconds (0.2% of data)
- RTs > 250.0 seconds (0.06% of data)

Before we run our model, we have to recover the present/absent responses made for each trial (rather than accuracy)

```{r}
d$response <- NA
d$response[which(d$targ=="present" & d$accuracy==1)] = 1
d$response[which(d$targ=="present" & d$accuracy==0)] = 0
d$response[which(d$targ=="absent" & d$accuracy==1)] = 0
d$response[which(d$targ=="absent" & d$accuracy==0)] = 1
```

Finally, save the data before modelling.

```{r}
write_csv(d, "../cluster/data_for_model.csv")
```


# Modelling

## Forumla

```{r}
formula <- bf(rt | dec(response) ~ 
              # the first part is the linear predictor for drift rate
              0 + targ:trial  + targ:trial:difficulty  + 
                (0 + targ:trial + targ:trial:difficulty|p|observer), 
              # bs: boundary separation 
              bs ~ 0 + trial + (0 + trial|p|observer), 
              # ndt: non-decision time
              ndt ~ 0 + trial + (1|p|observer),
              # bias: starting bias for TA or TP
              bias ~ 0 + trial + (0 + trial|p|observer))
```

## Prior

Print out list of all priors required for our formula, and any defaults that are already in place:
\tiny
```{r, eval=FALSE}
get_prior(formula,
          data = d, 
          family = wiener(link_bs = "identity", 
                          link_ndt = "identity", 
                          link_bias = "identity"))
```

Specify some of our own priors for the fixed effects:

\normalsize
```{r}
prior <- c(
  prior("normal(0, 3)", class = "b"),
  set_prior("normal(3, 3)", class = "b", dpar = "bs", lb = 0),
  set_prior("normal(0.2, 0.1)", class = "b", dpar = "ndt", lb = 0),
  set_prior("normal(0.5, 0.2)", class = "b", dpar = "bias", lb = 0, ub = 1))
```


```{r, echo = FASLE}
make_stancode(formula, 
              family = wiener(link_bs = "identity", 
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = d, 
              prior = prior)

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity", 
                                         link_ndt = "identity",
                                         link_bias = "identity"),
                         data = d, prior = prior)
str(tmp_dat, 1, give.attr = FALSE)
```

We also need to set some starting values, as the model fitting is a wee bit brittle sometimes:

```{r}
initfun <- function() {
  list(
    b = rnorm(tmp_dat$K),
    b_bs = runif(tmp_dat$K_bs, 1, 2),
    b_ndt = runif(tmp_dat$K_ndt, 0.01, 0.1),
    b_bias = rnorm(tmp_dat$K_bias, 0.5, 0.1),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    L_1 = diag(tmp_dat$M_1)
  )
}
```


Now make some prior predictions using just the fixed effects

```{r, eval = FALSE}
dp <- d %>%modelr::data_grid( trial, targ, difficulty, rt = 1, response = 1)

prior_formula <- bf(rt | dec(response) ~ 
              # the first part is the linear predictor for drift rate
              0 + targ:trial  + targ:trial:difficulty, 
              # bs: boundary separation 
              bs ~ 0 + trial, 
              # ndt: non-decision time
              ndt ~ 0 + trial,
              # bias: starting bias for TA or TP
              bias ~ 0 + trial)

prior_wiener <- brm(prior_formula,
                  data = dp,
                  family = wiener(link_bs = "identity",
                                  link_ndt = "identity",
                                  link_bias = "identity"),
                  prior = prior,
                  sample_prior = "only",
                  inits = initfun,
                  iter = 1000,
                  chains = 4, cores = 1,
                  control = list(max_treedepth = 15))
saveRDS(prior_wiener, "models/prior.model")

```

```{r, echo = FALSE}
prior_wiener <- readRDS("models/prior.model")
```

```{r}
NPRED <- 10
pred_wiener <- predict(newdata = dp,
                       prior_wiener, 
                       summary = FALSE, 
                       negative_rt = TRUE, 
                       nsamples = NPRED,
                       re_formula = NULL,
                       cores = 1)

rownames(pred_wiener) = paste("p", 1:10, sep = "_")

cbind(dp, t(pred_wiener)) %>% as_tibble() %>%
  pivot_longer(starts_with("p_"), names_to = "iter", values_to = "prt") %>%
  mutate(prt = if_else(targ=="absent", -prt, prt),
         pac = if_else(prt<0, 0, 1)) -> dp

dp %>%
  group_by(trial, targ, iter) %>%
  summarise(acc = mean(pac)) %>%
  median_hdci(acc)

dp %>%
  group_by(trial, targ) %>%
  median_hdci(abs(prt))

dp %>% ggplot(aes(x = abs(prt))) + geom_density() + 
  geom_density(data = d, aes(x=rt), colour = "blue") + 
  scale_x_log10()

```


```{r}

## plot accuracy
d %>% mutate(
  p_rt = if_else(targ == "absent", -p, p),
  p_re = if_else(p < 0, 0, 1),
  p_ac = if_else((targ=="present")==p_re, 1, 0)) -> d


d %>% mutate(rt = if_else(response == 0, -rt, rt)) %>%
  pivot_longer(c(rt, p), names_to = "measure", values_to = "rt") %>%
  ggplot(aes(x = rt, fill = measure)) + 
  geom_vline(xintercept = 0) + 
  geom_density(alpha = 0.3) + 
  scale_x_continuous(limits = c(-10, 10)) + 
  facet_wrap(targ ~ trial, scales="free")


```


